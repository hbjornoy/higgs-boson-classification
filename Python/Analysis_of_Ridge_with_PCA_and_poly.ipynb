{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression with pca and poly\n",
    "- Testing different cleanings\n",
    "- Testing different pca eleiminations\n",
    "- testing different degrees\n",
    "- testing different lambdas\n",
    "\n",
    "\n",
    "...try removing some features in PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# own functions\n",
    "import proj1_helpers as P1H\n",
    "import dataprocessing as DP\n",
    "import methods as ME\n",
    "import cross_validation as CV\n",
    "\n",
    "#constants\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_y, orig_x, orig_ids = P1H.load_csv_data(train_path, sub_sample=True) #remember to switch of subsample when running it \"for real\"\n",
    "pred_y, pred_x, pred_ids = P1H.load_csv_data(test_path, sub_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To provide clarity to which data I am processing\n",
    "x = np.copy(orig_x)\n",
    "\n",
    "# Cleans values that are -999 to zero, mean and median\n",
    "no_clean = np.copy(orig_x)\n",
    "clean_zero = DP.clean_data(x)\n",
    "clean_mean = DP.clean_data(x, replace_no_measure_with_mean=True)\n",
    "clean_medi = DP.clean_data(x, replace_no_measure_with_median=True)\n",
    "\n",
    "# Make array to test for later\n",
    "cleanDataArray = [no_clean, clean_zero, clean_mean, clean_medi]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on data then polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# could make into a function that takes in different highest degree and number of PCA dimensions\n",
    "\n",
    "degrees=(3,5,7,9,11,13,15)\n",
    "\n",
    "# So one can append new dataset to\n",
    "pca_then_poly_matrix_diff_clean = np.zeros((len(cleanDataArray),len(degrees)), dtype=object)\n",
    "\n",
    "# make phi for one and one cleaned dataset\n",
    "for i, dataset in enumerate(cleanDataArray):\n",
    "\n",
    "    # making new polynomial feature for the given dataset\n",
    "    for j, degree in enumerate(degrees):\n",
    "        \n",
    "        pc = DP.pca(dataset, 30)[0]\n",
    "        phi = DP.build_poly(pc, degree)\n",
    "        pca_then_poly_matrix_diff_clean[i,j] = phi[:,:]\n",
    "        \n",
    "# Making an matrix that are going to have\n",
    "# rows = different methods to clean dataset\n",
    "# columns = different degrees\n",
    "print(pca_then_poly_matrix_diff_clean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.copy(orig_y)\n",
    "y_log = DP.classify(y)\n",
    "\n",
    "k_folds = 5\n",
    "#parameters for ridge regression\n",
    "lambda_=0.000000001\n",
    "#parameters for logistic regression\n",
    "max_iters = 5\n",
    "gamma = 0.01\n",
    "\n",
    "phis = np.copy(pca_then_poly_matrix_diff_clean)\n",
    "avg_predictions_of_phis = np.zeros((phis.shape[0], phis.shape[1]), dtype=object)\n",
    "\n",
    "for i in range(phis.shape[0]):\n",
    "    for j, phi in enumerate(phis[i,:]):\n",
    "        initial_w = np.zeros(((phi.shape[1], 1)))\n",
    "        avg_loss, losses, avg_preds, pred_acc_percents = CV.cross_validation(ME.ridge_regression, y, phi, k_folds, lambda_)\n",
    "        #avg_loss, losses, avg_preds, pred_acc_percents = CV.cross_validation(ME.logistic_regression, y_log, phi, k_folds, initial_w, max_iters, gamma)\n",
    "        avg_predictions_of_phis[i,j] = avg_preds\n",
    "        \n",
    "\n",
    "# should become a matrix with containing average predicitions of different data and polynomials\n",
    "print(avg_predictions_of_phis.shape)\n",
    "#plt.plot(degrees,avg_predictions_of_phis[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of different cleaning tactics vs highest polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(degrees,avg_predictions_of_phis[0,:],color='b', marker='*', label=\"no_clean\")\n",
    "plt.plot(degrees,avg_predictions_of_phis[1,:],color='r', marker='*', label=\"clean_zero\")\n",
    "plt.plot(degrees,avg_predictions_of_phis[2,:],color='g', marker='*', label=\"clean_mean\")\n",
    "plt.plot(degrees,avg_predictions_of_phis[3,:],color='m', marker='*', label=\"clean_median\")\n",
    "\n",
    "leg = plt.legend(loc=1, shadow=True)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.15,0.25])\n",
    "plt.show\n",
    "print(np.min(avg_predictions_of_phis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final testing of spesific algorithm on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_orig_y, w_orig_x, w_orig_ids = load_csv_data(train_path, sub_sample=False) #remember to switch of subsample when running it \"for real\"\n",
    "w_pred_y, w_pred_x, w_pred_ids = load_csv_data(test_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combining train.csv and test.csv\n",
    "w_ALL_THE_DATA = np.vstack((w_orig_x, w_pred_x))\n",
    "print(w_orig_x.shape, w_pred_x.shape)\n",
    "print(w_ALL_THE_DATA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean data\n",
    "w_clean_mean = DP.clean_data(x, replace_no_measure_with_mean=True)\n",
    "\n",
    "# Using PCA on the data before split so that the train and testset get the same eigenvectors\n",
    "w_pc = DP.pca(w_ALL_THE_DATA, 30)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# polynomial degree 7\n",
    "w_degree = 7\n",
    "w_phi = DP.build_poly(w_pc, w_degree)\n",
    "\n",
    "print(w_phi.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# have to know where to divide between the train.csv and test.csv\n",
    "split = w_orig_x.shape[0]\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "#parameters for ridge regression\n",
    "lambda_=0.000000000001\n",
    "\n",
    "# crossvalidation with ridgeregression on w_phi[]\n",
    "avg_loss, losses, avg_preds, pred_acc_percents = CV.cross_validation(ME.ridge_regression, w_orig_y, w_phi[:split], k_folds, lambda_)\n",
    "\n",
    "print(avg_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue for actual delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters for ridge regression\n",
    "lambda_=0.000000000001\n",
    "rmse, weights = ME.alt_ridge_regression(w_orig_y, w_phi[:split], lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "predictions = P1H.predict_labels(weights, w_phi[split:])\n",
    "print(predictions[1:25])\n",
    "print(predictions.mean())\n",
    "print(w_orig_y.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates a file with the name you want in the folder of the Python-file. just look in the folder\n",
    "name = 'pca_poly7_ridge.csv'\n",
    "P1H.create_csv_submission(w_pred_ids, predictions, name)\n",
    "\n",
    "#\n",
    "# THE END - deliver the file to kaggle\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
