{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression with pca and poly\n",
    "- Testing different cleanings\n",
    "- Testing different pca eleiminations\n",
    "- testing different degrees\n",
    "- testing different lambdas\n",
    "\n",
    "\n",
    "...try removing some features in PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# own functions\n",
    "import proj1_helpers as P1H\n",
    "import dataprocessing as DP\n",
    "import methods as ME\n",
    "import cross_validation as CV\n",
    "\n",
    "#constants\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_y, orig_x, orig_ids = P1H.load_csv_data(train_path, sub_sample=False) #remember to switch of subsample when running it \"for real\"\n",
    "pred_y, pred_x, pred_ids = P1H.load_csv_data(test_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To provide clarity to which data I am processing\n",
    "x = np.copy(orig_x)\n",
    "\n",
    "# Cleans values that are -999 to zero, mean and median\n",
    "no_clean = np.copy(orig_x)\n",
    "clean_zero = DP.clean_data(x)\n",
    "clean_mean = DP.clean_data(x, replace_no_measure_with_mean=True)\n",
    "clean_medi = DP.clean_data(x, replace_no_measure_with_median=True)\n",
    "\n",
    "# Make array to test for later\n",
    "cleanDataArray = [no_clean, clean_zero, clean_mean, clean_medi]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on data then polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# could make into a function that takes in different highest degree and number of PCA dimensions\n",
    "\n",
    "degrees = (3,5,7,9,11,13,15,20)\n",
    "pc_keeps = (27,28,29,30)\n",
    "\n",
    "# So one can append new dataset to\n",
    "clean_pc_poly = np.zeros((len(cleanDataArray),len(pc_keeps), len(degrees)), dtype=object)\n",
    "\n",
    "# make phi for one and one cleaned dataset\n",
    "for i, dataset in enumerate(cleanDataArray):\n",
    "    \n",
    "    # Doing PCA for different cleanings and reducing dimensionality too pc_keeps[j]\n",
    "    for j, keep in enumerate(pc_keeps):\n",
    "    \n",
    "        pc = DP.pca(dataset, keep)[0]\n",
    "        \n",
    "        # making new polynomial feature for the given dataset\n",
    "        for k, degree in enumerate(degrees):\n",
    "\n",
    "            phi = DP.build_poly(pc, degree)\n",
    "            clean_pc_poly[i,j,k] = phi[:,:]\n",
    "        \n",
    "# Making an matrix that are going to have\n",
    "# rows = different methods to clean dataset\n",
    "# columns = different degrees\n",
    "print(clean_pc_poly.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.copy(orig_y)\n",
    "y_log = DP.classify(y)\n",
    "\n",
    "k_folds = 5\n",
    "#parameters for ridge regression\n",
    "lambdas=np.logspace(-11,1,10)\n",
    "#parameters for logistic regression\n",
    "max_iters = 5\n",
    "gamma = 0.01\n",
    "\n",
    "phis = np.copy(clean_pc_poly)\n",
    "avg_predictions_of_phis = np.zeros((len(lambdas), phis.shape[0], phis.shape[1], phis.shape[2]), dtype=object)\n",
    "\n",
    "for l, lambda_ in enumerate(lambdas):\n",
    "    for i in range(phis.shape[0]):\n",
    "        for j in range(phis.shape[1]):\n",
    "            for k in range(phis.shape[2]):\n",
    "                phi = phis[i,j,k]\n",
    "                initial_w = np.zeros(((phi.shape[1], 1)))\n",
    "                avg_loss, losses, avg_preds, pred_acc_percents = CV.cross_validation(ME.ridge_regression, y, phi, k_folds, lambda_)\n",
    "                #avg_loss, losses, avg_preds, pred_acc_percents = CV.cross_validation(ME.logistic_regression, y_log, phi, k_folds, initial_w, max_iters, gamma)\n",
    "                avg_predictions_of_phis[l,i,j,k] = avg_preds\n",
    "\n",
    "\n",
    "# should become a matrix with containing average predicitions of different data and polynomials\n",
    "print(avg_predictions_of_phis.shape)\n",
    "#plt.plot(degrees,avg_predictions_of_phis[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of different cleaning tactics vs highest polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot for clean_mean\n",
    "# use the lambda you want to plot lambdas=np.logspace(-8,0,10)\n",
    "l = 2\n",
    "print('lamdba: ', lambdas[l])\n",
    "#type of data cleaned\n",
    "c = 2 \n",
    "print('clean_Strategy:', c)\n",
    "print('0=noClean, 1=cleanZero, 2=cleanMean, 3=cleanMedian')\n",
    "# pc_keeps = (15,18,21,24,27,30)\n",
    "plt.plot(degrees,avg_predictions_of_phis[l,c,0,:],color='b', marker='*', label=\"pca 15\")\n",
    "plt.plot(degrees,avg_predictions_of_phis[l,c,1,:],color='r', marker='*', label=\"pca 18\")\n",
    "plt.plot(degrees,avg_predictions_of_phis[l,c,2,:],color='g', marker='*', label=\"pca 21\")\n",
    "plt.plot(degrees,avg_predictions_of_phis[l,c,3,:],color='m', marker='*', label=\"pca 24\")\n",
    "plt.plot(degrees,avg_predictions_of_phis[l,c,4,:],color='y', marker='*', label=\"pca 27\")\n",
    "plt.plot(degrees,avg_predictions_of_phis[l,c,5,:],color='k', marker='*', label=\"pca 30\")\n",
    "\n",
    "leg = plt.legend(loc=1, shadow=True)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.15,0.40])\n",
    "plt.show\n",
    "print(np.min(avg_predictions_of_phis[l,c,:,:]))\n",
    "print(np.min(avg_predictions_of_phis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final testing of spesific algorithm on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_orig_y, w_orig_x, w_orig_ids = P1H.load_csv_data(train_path, sub_sample=False) #remember to switch of subsample when running it \"for real\"\n",
    "w_pred_y, w_pred_x, w_pred_ids = P1H.load_csv_data(test_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combining train.csv and test.csv\n",
    "w_ALL_THE_DATA = np.vstack((w_orig_x, w_pred_x))\n",
    "print(w_orig_x.shape, w_pred_x.shape)\n",
    "print(w_ALL_THE_DATA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean data\n",
    "w_clean_mean = DP.clean_data(x, replace_no_measure_with_mean=True)\n",
    "\n",
    "# Using PCA on the data before split so that the train and testset get the same eigenvectors\n",
    "w_pca_degree = 30\n",
    "w_pc = DP.pca(w_ALL_THE_DATA, w_pca_degree)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# polynomial degree\n",
    "w_degree = 10\n",
    "w_phi = DP.build_poly(w_pc, w_degree)\n",
    "\n",
    "print(w_phi.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# have to know where to divide between the train.csv and test.csv\n",
    "split = w_orig_x.shape[0]\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_folds = 5\n",
    "#parameters for ridge regression\n",
    "lambda_=5.98484250319e-07\n",
    "\n",
    "# crossvalidation with ridgeregression on w_phi[]\n",
    "avg_loss, losses, avg_preds, pred_acc_percents = CV.cross_validation(ME.ridge_regression, w_orig_y, w_phi[:split], k_folds, lambda_)\n",
    "\n",
    "print(avg_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue for actual delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters for ridge regression\n",
    "lambda_= 5.98484250319e-05\n",
    "rmse, weights = ME.alt_ridge_regression(w_orig_y, w_phi[:split], lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "predictions = P1H.predict_labels(weights, w_phi[split:])\n",
    "print(predictions[1:25])\n",
    "print(predictions.mean())\n",
    "print(w_orig_y.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates a file with the name you want in the folder of the Python-file. just look in the folder\n",
    "name = 'pca30_poly10_ridge.csv'\n",
    "P1H.create_csv_submission(w_pred_ids, predictions, name)\n",
    "\n",
    "#\n",
    "# THE END - deliver the file to kaggle\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
