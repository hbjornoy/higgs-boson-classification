{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step\n",
    "\n",
    "In this script, the \"local optima leads to global optima\" assumption. \n",
    "The data is cleaned, using clean zero, mean and meadian. Then it is normalized. \n",
    "Using crossvalidation, the best method to clean data is selected. \n",
    "\n",
    "Thereafter, the data is transformed using PCA analysis with the first 30-20 components. \n",
    "Using crossvalidation, the number of principal components that generates best results is selected. \n",
    "\n",
    "Thereafter, the data is transformed using polynomials, and a optimal combination of lambda and degree is selected. \n",
    "\n",
    "Finally, a kaggle-submission is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and personal libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# standard libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# own functions\n",
    "\n",
    "import proj1_helpers as P1H\n",
    "import dataprocessing as DP\n",
    "import implementations as ME\n",
    "import cross_validation as CV\n",
    "from grad_loss import*\n",
    "\n",
    "#constants\n",
    "train_path = 'train.csv'\n",
    "test_path = 'test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_y, orig_x, orig_ids = load_csv_data(train_path, sub_sample=True) \n",
    "pred_y, pred_x, pred_ids = load_csv_data(test_path, sub_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking training and test data together to easily preform the same transformations on both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_x = np.vstack((orig_x, pred_x))\n",
    "\n",
    "# value we split the all_x on before testing\n",
    "split_coord = len(orig_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preforming the three different ways of cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To provide clarity to which data that is processed\n",
    "x = np.copy(all_x)\n",
    "\n",
    "# Cleans values that are -999 to zero, mean and median\n",
    "no_clean = np.copy(x)\n",
    "clean_zero = DP.clean_data(x)\n",
    "clean_mean = DP.clean_data(x, replace_no_measure_with_mean=True)\n",
    "clean_medi = DP.clean_data(x, replace_no_measure_with_median=True)\n",
    "\n",
    "# Make array to test for later\n",
    "cleanDataArray = [no_clean, clean_zero, clean_mean, clean_medi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Normalizing data:\n",
    "normalizedDataArray=[]\n",
    "for i, data in enumerate(cleanDataArray):\n",
    "    normalizedDataArray.append(DP.normalize(data))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the different ways of cleaning data, using a 5 fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is lambda 2.33572146909e-05\n",
      "this is average losses:  [0.81817196959179417, 0.80883503277725899, 0.81225633044988821, 0.81707134549374738]\n",
      "this is average prediction error: [0.2522, 0.2444, 0.2476, 0.2496]\n"
     ]
    }
   ],
   "source": [
    "lambda_=2.33572146909e-05 #taken from exploration of basic methods\n",
    "k_folds=5\n",
    "avg_losses=[]\n",
    "avg_preds_all=[]\n",
    "for data in normalizedDataArray:\n",
    "    avg_loss, losses, avg_preds, pred_acc_percents = CV.cross_validation(ME.ridge_regression, orig_y, data[:split_coord,:], k_folds, lambda_)\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_preds_all.append(avg_preds)\n",
    "print(\"this is lambda\", lambda_)\n",
    "print(\"this is average losses: \",avg_losses)\n",
    "print(\"this is average prediction error:\",avg_preds_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosing the method of cleaning that minimizes loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "##Based on this, we choose to continue with the data where all missing values are replaced by 0. \n",
    "\n",
    "chosenData=normalizedDataArray[np.argmin(avg_losses)]\n",
    "print(np.argmin(avg_losses))\n",
    "\n",
    "#Want to test what happen if we choose clean mean: \n",
    "#chosenData=normalizedDataArray[2]\n",
    "\n",
    "#Want to test what happen if we choose clean median: \n",
    "#chosenData=normalizedDataArray[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing PCA, keeping different number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Now we want to performe PCA on the chosen data\n",
    "numberOfDimensions=(30,29,28,27,26,25,24,23,22,21,20)\n",
    "pcas=[]\n",
    "for i, degree in enumerate(numberOfDimensions):\n",
    "    pca_i=DP.pca(chosenData,degree)[0]\n",
    "    pcas.append(pca_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the different dimensions, using a 5 fold cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is lambda 2.33572146909e-05\n",
      "this is average losses:  [0.86957757215410347, 0.8695775722204877, 0.87160488127106439, 0.87174138193975514, 0.87184296963476027, 0.87890148652423172, 0.88062914093091427, 0.88093517590255066, 0.88820784924404061, 0.88903865691775974, 0.89003075433178724]\n",
      "this is average prediction error: [0.2692, 0.2692, 0.2666, 0.2672, 0.2682, 0.268, 0.2694, 0.27, 0.2762, 0.2752, 0.2764]\n"
     ]
    }
   ],
   "source": [
    "lambda_=2.33572146909e-05 #taken from exploration of basic methods\n",
    "k_folds=5\n",
    "avg_losses=[]\n",
    "avg_preds_all=[]\n",
    "for data in pcas:\n",
    "    avg_loss, losses, avg_preds, pred_acc_percents = CV.cross_validation(ME.ridge_regression, orig_y, data[:split_coord,:], k_folds, lambda_)\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_preds_all.append(avg_preds)\n",
    "print(\"this is lambda\", lambda_)\n",
    "print(\"this is average losses: \",avg_losses)\n",
    "print(\"this is average prediction error:\",avg_preds_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosing the number of dimensions that minimizes loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "chosenData=pcas[np.argmin(avg_losses)]\n",
    "print(np.argmin(avg_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Finding the best combination of polynomial degree and lambda_ using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degrees=(3,4,5,6,7,8,9)\n",
    "lambdas=np.logspace(-9,1,15)\n",
    "min_loss=1000;\n",
    "min_degree=0;\n",
    "min_lambda=0\n",
    "max_acc=0\n",
    "avg_losses=np.zeros((len(degrees),len(lambdas)))\n",
    "avg_acc=np.zeros((len(degrees),len(lambdas)))\n",
    "for d,degree in enumerate(degrees):\n",
    "    phi=DP.build_poly(chosenData[:split_coord,:],degree)\n",
    "    for l,lambda_ in enumerate(lambdas):\n",
    "        avg_loss, losses, avg_preds, pred_acc_percents = CV.cross_validation(ME.ridge_regression, orig_y, phi, k_folds, lambda_)\n",
    "        avg_losses[d,l]=avg_loss\n",
    "        avg_acc[d,l]=avg_preds\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss=avg_loss\n",
    "            min_degree=degree\n",
    "            min_lambda=lambda_\n",
    "            max_acc=avg_preds\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average minmal loss is:  0.748601946659 which is found using a polynomial of degree  9  with lambda_= 1e-09\n",
      "The average prediction error when compared to the real values is:  0.1958\n"
     ]
    }
   ],
   "source": [
    "print(\"The average minmal loss is: \", min_loss, \"which is found using a polynomial of degree \",min_degree, \" with lambda_=\", min_lambda)\n",
    "print(\"The average prediction error when compared to the real values is: \",max_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosing the combination that minimizes loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosenData=DP.build_poly(chosenData,min_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss,w=ME.ridge_regression(orig_y,chosenData[:split_coord,:],min_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted=P1H.predict_labels(w,chosenData[split_coord:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name=\"step_by_step.csv\"\n",
    "#name=\"step_by_step_clean_mean.csv\"\n",
    "#name=\"step_by_step_clean_median.csv\"\n",
    "\n",
    "create_csv_submission(pred_ids, y_predicted, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle score: \n",
    "- clean zero (step_by_step): 0.77544\n",
    "- clean mean: 0.79162\n",
    "- clean median: 0.79344 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
